{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d41e60-df83-4669-a91b-3f4014025e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models, Tokenizer, and Label Encoder loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved models\n",
    "cnn_model = load_model(\"cnn_model_sh.keras\")\n",
    "lstm_model = load_model(\"lstm_model_sh.keras\")\n",
    "rnn_model = load_model(\"rnn_model_sh.keras\")\n",
    "meta_model = load_model(\"meta_model_sh.keras\")\n",
    "\n",
    "# Load the tokenizer\n",
    "with open(\"tokenizer_sh.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Load the label encoder\n",
    "with open(\"label_encoder_sh.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "print(\"Models, Tokenizer, and Label Encoder loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fd7ef19-b8f8-4dbf-ac17-fd64fb141dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import json\n",
    "nlp = spacy.blank(\"hi\")  # blank model for tokenization\n",
    "with open(\"safe_stopwords_hi.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hindi_stopwords = set(json.load(f))\n",
    "\n",
    "# Only remove non-Hindi characters and unwanted symbols\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    # Remove only English letters, special symbols; keep Hindi and digits if needed\n",
    "    text = re.sub(r'[a-zA-Z]', '', text)\n",
    "    text = re.sub(r'[^\\u0900-\\u097F0-9\\s]', '', text)  # Keep Hindi chars only\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    #return text\n",
    "# 2. Tokenizer using spaCy\n",
    "    doc = nlp(text)\n",
    "    tokens= [token.text for token in doc if token.text not in hindi_stopwords and token.text.strip() != \"\"]\n",
    "    return \" \".join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5dfc072-27ab-45d0-adff-4a49b064d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def predict_category(headline):\n",
    "    # Step 1: Preprocess the input headline\n",
    "    headline = preprocess_text(headline)\n",
    "    sequence = tokenizer.texts_to_sequences([headline])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=200)\n",
    "\n",
    "    # Step 2: Get predictions from base models\n",
    "    cnn_pred = cnn_model.predict(padded_sequence)\n",
    "    lstm_pred = lstm_model.predict(padded_sequence)\n",
    "    rnn_pred = rnn_model.predict(padded_sequence)\n",
    "\n",
    "    # Step 3: Stack predictions\n",
    "    stacked_pred = np.concatenate([cnn_pred, lstm_pred, rnn_pred], axis=1)\n",
    "\n",
    "    # Step 4: Meta-model final prediction\n",
    "    final_pred = meta_model.predict(stacked_pred)\n",
    "\n",
    "    # Step 5: Decode prediction\n",
    "    predicted_class = label_encoder.inverse_transform([np.argmax(final_pred)])\n",
    "    confidence = float(np.max(final_pred))\n",
    "\n",
    "    return predicted_class[0], round(confidence, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edb7331c-848b-431b-81fd-02619c306968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "The predicted category for the headline is: ('Entertainment', 0.997)(99.70% confidence)\n"
     ]
    }
   ],
   "source": [
    "# Example headline\n",
    "headline = \"नयी फिल्म 'पठान' ने बॉक्स ऑफिस पर मचाया धमाल\"\n",
    "\n",
    "# Get the predicted category\n",
    "category, conf=  predicted_category = predict_category(headline)\n",
    "\n",
    "# Print the predicted category\n",
    "print(f\"The predicted category for the headline is: {predicted_category}({conf*100:.2f}% confidence)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15197e93-a2dd-4e54-b9fc-673188f949dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask\n",
      "  Using cached flask-3.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in e:\\news classification\\myenv\\lib\\site-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in e:\\news classification\\myenv\\lib\\site-packages (from flask) (3.1.6)\n",
      "Collecting itsdangerous>=2.2 (from flask)\n",
      "  Using cached itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in e:\\news classification\\myenv\\lib\\site-packages (from flask) (8.1.8)\n",
      "Collecting blinker>=1.9 (from flask)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: colorama in e:\\news classification\\myenv\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\news classification\\myenv\\lib\\site-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
      "Using cached flask-3.1.0-py3-none-any.whl (102 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: itsdangerous, blinker, flask\n",
      "Successfully installed blinker-1.9.0 flask-3.1.0 itsdangerous-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba5d0c5-01a6-4bfd-8211-0ca94be95f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
